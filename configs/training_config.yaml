model:
  base_checkpoint: "gpt-2"
  fine_tune_epochs: 3
  batch_size: 8
  learning_rate: 2e-5

data:
  train_split: 0.8
  seed: 42

output:
  save_dir: "checkpoints/"
